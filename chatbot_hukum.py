# -*- coding: utf-8 -*-
"""chatbot-hukum.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A9fR1KtZ_BH7rDbx_INPze7ghUE8_OB8

# Load Datset
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/pp51_v2.csv')
print(df.head())

"""# Library yang Dibutuhkan"""

!pip install transformers accelerate bitsandbytes gradio langchain sentence-transformers chromadb peft trl datasets

"""# Preprocessing"""

from sentence_transformers import SentenceTransformer

# Gunakan model ringan dan efisien, cocok untuk kebutuhan hukum
model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

# Kita akan encode jawaban sebagai dokumen referensi
documents = df['jawaban'].tolist()

# Ubah ke bentuk embedding
document_embeddings = model.encode(documents, convert_to_tensor=True)

import chromadb
from chromadb.utils import embedding_functions

# Inisialisasi ChromaDB (lokal, tidak pakai server)
chroma_client = chromadb.Client()

# Buat collection baru bernama "peraturan_upah"
collection = chroma_client.create_collection(name="peraturan_upah")

# Load kembali model Sentence-Transformer untuk digunakan sebagai embedder
embedder = embedding_functions.SentenceTransformerEmbeddingFunction(model_name="all-MiniLM-L6-v2")

# Tambahkan data ke collection
for i, (pertanyaan, jawaban) in enumerate(zip(df['pertanyaan'], df['jawaban'])):
    collection.add(
        documents=[jawaban],
        metadatas=[{"pertanyaan": pertanyaan}],
        ids=[f"doc-{i}"]
    )

"""# Membuat Fungsi"""

def jawab_pertanyaan(user_question):
    # Ambil 1 dokumen paling relevan berdasarkan pertanyaan
    results = collection.query(
        query_texts=[user_question],
        n_results=1
    )
    if results["documents"]:
        return results["documents"][0][0]
    else:
        return "Maaf, saya tidak menemukan jawaban yang sesuai dari peraturan yang tersedia."

import gradio as gr

interface = gr.Interface(
    fn=jawab_pertanyaan,
    inputs="text",
    outputs="text",
    title="Chatbot Hukum: PP No. 51 Tahun 2023",
    description="Tanyakan tentang pengupahan berdasarkan PP 51 Tahun 2023"
)

interface.launch()

"""# Mengintegrasikan Mistral 7B"""

from huggingface_hub import login

# Ganti dengan token kamu
login(token="") # Sesuaikan dengan Token yang didapat dari Hugging Face

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_id = "mistralai/Mistral-7B-Instruct-v0.1"  # atau model kamu sendiri kalau sudah fine-tuned

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    torch_dtype=torch.float16,
    load_in_4bit=True  # hemat memori
)

def jawab_pertanyaan_rag(user_question):
    # Ambil dokumen hukum relevan dari ChromaDB
    results = collection.query(
        query_texts=[user_question],
        n_results=2  # Ambil lebih dari 1 untuk konteks yang lebih kaya
    )

    if not results["documents"]:
        return "Maaf, saya tidak menemukan jawaban dari peraturan yang tersedia."

    context = "\n\n".join([doc[0] for doc in results["documents"]])

    # Bangun prompt untuk Mistral
    prompt = f"""
Kamu adalah asisten hukum cerdas. Jawablah pertanyaan berikut berdasarkan dokumen hukum yang disediakan.

Dokumen Hukum:
\"\"\"
{context}
\"\"\"

Pertanyaan:
{user_question}

Jawaban:
"""

    # Tokenisasi & Inferensi dari Mistral
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    output = model.generate(**inputs, max_new_tokens=256, do_sample=True, temperature=0.7)
    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)

    # Ambil hanya jawaban bagian akhir
    if "Jawaban:" in generated_text:
        return generated_text.split("Jawaban:")[-1].strip()
    else:
        return generated_text.strip()

import gradio as gr

interface = gr.Interface(
    fn=jawab_pertanyaan_rag,
    inputs="text",
    outputs="text",
    title="Chatbot Hukum: PP No. 51 Tahun 2023 + Mistral 7B",
    description="Tanyakan tentang hukum pengupahan sesuai PP No. 51 Tahun 2023. Didukung pencarian hukum + LLM."
)

interface.launch(share=True)

"""# Evaluasi"""

!pip install evaluate

!pip install evaluate rouge_score sacrebleu nltk

import nltk
nltk.download('punkt')

from nltk.tokenize import word_tokenize
import evaluate

bleu = evaluate.load("bleu")

generated = ["UMP ditetapkan oleh gubernur setiap tahun."]
reference = [["UMP setiap tahun ditetapkan oleh gubernur."]]  # <- format penting: list dalam list

# BLEU langsung (tanpa tokenisasi manual)
bleu_result = bleu.compute(predictions=generated, references=reference)
print("BLEU:", bleu_result)

import evaluate

# Load evaluator
rouge = evaluate.load("rouge")

# Contoh prediksi dan referensi
generated = ["UMP ditetapkan oleh gubernur setiap tahun."]
reference = ["UMP setiap tahun ditetapkan oleh gubernur."]

# Evaluasi ROUGE
rouge_result = rouge.compute(predictions=generated, references=reference)

# Cetak hasilnya
for metric, value in rouge_result.items():
    print(f"{metric}: {value:.4f}")

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Misal jawaban relevan = 1, tidak relevan = 0
y_true = [1, 1, 0, 1]
y_pred = [1, 0, 0, 1]

print("Accuracy:", accuracy_score(y_true, y_pred))
print("Precision:", precision_score(y_true, y_pred))
print("Recall:", recall_score(y_true, y_pred))
print("F1 Score:", f1_score(y_true, y_pred))
